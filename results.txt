$ python train.py  --gpu 0 --alg rc1
GPU: 0
# n_units: 128
# n_hidden: 256
# Length of action sequence: 6
# RAM model: ram0
# Reinforce Algorithm: rc1
# scale: 0.3
# sigma: 0.1
# baseline mode: adaptive
# loss coefficient: 1.0
# baseline loss coefficient: 1.0
# Optimization algorithm: adam
# Minibatch-size: 500
# epoch: 1000

epoch       main/c      main/r      main/loss   main/accuracy  val/main/c  val/main/r  val/main/loss  val/main/accuracy  test/main/c  test/main/r  test/main/loss  test/main/accuracy
1           1.99326     -5.00645    -2.45065    0.27255        1.63324     -3.04549    -0.927681      0.4239             1.50024      -4.37409     -2.37657        0.4554              
2           1.5138      -2.17894    -0.241837   0.46785        1.54333     -2.92859    -1.01568       0.4599             1.35368      -4.02156     -2.32154        0.5408              
3           1.09737     -2.34747    -0.834611   0.624383       1.03351     -0.611807   0.774835       0.6473             0.865461     -0.690392    0.495426        0.7167              
4           0.78863     0.38749     1.51463     0.734033       0.619173    1.44337     2.40234        0.7931             0.54884      1.93409      2.79654         0.8248              
5           1.00229     -0.720596   0.636879    0.660533       0.97228     -1.30029    0.0600285      0.6688             0.95903      -3.15801     -1.86073        0.6654              
20          0.570442    -1.51543    -0.556684   0.804316       0.485154    -1.46641    -0.572743      0.8315             0.390527     -3.1394      -2.3704         0.867               
50          0.359353    -2.67072    -2.03398    0.870016       0.342182    -2.41519    -1.80907       0.8776             0.281491     -4.12226     -3.5964         0.9037              
100         0.361867    -1.73531    -1.14855    0.871283       0.355892    -1.614      -1.04393       0.8721             0.326278     -2.92348     -2.39552        0.8867              
200         0.238974    -1.59348    -1.15281    0.912683       0.225703    -1.19714    -0.775078      0.918              0.169878     -2.78881     -2.44143        0.9447              
300         0.261248    -1.50916    -1.06384    0.904467       0.276062    -1.26796    -0.813638      0.901              0.213761     -1.82789     -1.4538         0.9245              
400         0.240619    -1.57021    -1.15698    0.913284       0.258959    -1.67545    -1.23684       0.9077             0.203087     -2.85041     -2.48193        0.9308              
500         0.260771    -1.40342    -0.965441   0.904267       0.27592     -1.46282    -1.00335       0.8984             0.20472      -2.73115     -2.35324        0.93                
600         0.253131    -1.73516    -1.28262    0.9068         0.267263    -1.669      -1.20343       0.9025             0.238703     -3.06158     -2.62683        0.9182              
700         0.247883    -1.20651    -0.770429   0.9095         0.279843    -1.21317    -0.74523       0.8998             0.227574     -2.35351     -1.94996        0.9163              
800         0.237095    -1.08542    -0.658467   0.911834       0.2625      -0.821472   -0.37648       0.9044             0.227402     -1.73637     -1.33691        0.9188              
900         0.269227    -1.13873    -0.675324   0.90115        0.275369    -0.978869   -0.521167      0.8994             0.239563     -1.82417     -1.42121        0.9153              


$ python train.py --gpu 0
GPU: 0
# n_units: 128
# n_hidden: 256
# Length of action sequence: 6
# RAM model: ram0
# Reinforce Algorithm: rc
# scale: 0.3
# sigma: 0.1
# baseline mode: adaptive
# loss coefficient: 1.0
# baseline loss coefficient: 1.0
# Optimization algorithm: adam
# Minibatch-size: 500
# epoch: 1000

epoch       main/c      main/r      main/loss   main/accuracy  val/main/c  val/main/r  val/main/loss  val/main/accuracy  test/main/c  test/main/r  test/main/loss  test/main/accuracy
1           2.22899     -0.235195   2.19786     0.17085        2.07278     0.410671    2.70818        0.2399             2.05359      1.32921      3.63026         0.2409              
2           1.85796     0.0909715   2.45711     0.321767       1.659       0.460948    2.72569        0.4008             1.66785      0.51103      2.79997         0.3729              
5           1.17912     0.12112     2.28537     0.5948         1.05385     0.0766363   2.06176        0.639              1.04261      -0.546017    1.55699         0.6478              
10          0.753785    0.0110496   1.63731     0.74305        0.670908    0.0725188   1.50554        0.7704             0.628522     -0.656137    0.793173        0.7837              
20          0.545733    -0.0306874  1.22314     0.809633       0.539218    -0.627054   0.608865       0.811              0.425209     -1.19932     -0.14929        0.8523              
100         0.389064    -0.013082   0.891392    0.859283       0.373268    0.111538    1.00805        0.8679             0.234329     0.168238     0.780399        0.9174              
200         0.318929    -0.0152827  0.737592    0.883767       0.337935    0.112068    0.961309       0.8812             0.206128     0.507474     1.02008         0.9271              
300         0.316012    -0.00793214  0.747601    0.885617       0.302129    0.121431    0.816174       0.8932             0.201259     -0.302585    0.239311        0.9276              
400         0.290988    -0.0196877  0.67466     0.89545        0.28912     -0.110602   0.603729       0.8983             0.175494     -0.247813    0.264371        0.9432              
500         0.279021    -0.0123268  0.653883    0.9001         0.29593     -0.175851   0.567212       0.8948             0.200546     -0.416854    0.163894        0.9311              
600         0.273106    0.000297529  0.654399    0.9013         0.270219    -0.186375   0.475874       0.9029             0.161038     -0.411412    0.0284534       0.9461              
700         0.283582    -0.0104559  0.662382    0.898717       0.295109    -0.254418   0.483541       0.8934             0.212834     -0.526092    0.0772896       0.9287              
800         0.277053    -0.0366658  0.655024    0.90255        0.290597    0.0702304   0.806866       0.8976             0.211166     -0.195939    0.417034        0.9294              
900         0.27482     -0.00193495  0.657398    0.901667       0.285659    -0.106217   0.617387       0.901              0.154336     -0.16961     0.249398        0.946               



$ python train.py  --gpu 0 --alg rc --coeffb 0.1
GPU: 0
# n_units: 128
# n_hidden: 256
# Length of action sequence: 6
# RAM model: ram0
# Reinforce Algorithm: rc
# scale: 0.3
# sigma: 0.1
# baseline mode: adaptive
# loss coefficient: 1.0
# baseline loss coefficient: 0.1
# Optimization algorithm: adam
# Minibatch-size: 500
# epoch: 1000

epoch       main/c      main/r      main/loss   main/accuracy  val/main/c  val/main/r  val/main/loss  val/main/accuracy  test/main/c  test/main/r  test/main/loss  test/main/accuracy
1           2.16761     -0.309987   1.88766     0.1995         1.86886     1.04537     2.96788        0.3371             1.86732      1.64464      3.5788          0.3146              
2           1.59517     0.284844    1.95635     0.441533       1.42443     -1.00525    0.533651       0.5017             1.4864       -2.37696     -0.744852       0.5026              
3           1.19909     0.254567    1.55143     0.593          1.01093     0.61731     1.7196         0.6571             1.06735      0.028628     1.20117         0.6389              
4           1.33744     -0.0049689  1.42869     0.5287         1.49892     -0.601469   0.965601       0.4573             1.32391      -0.404764    0.981764        0.5202              
5           1.2712      0.219859    1.59659     0.5646         1.07392     0.820832    1.9863         0.6299             0.987694     1.08108      2.15024         0.6591              
20          0.470145    -0.030737   0.505509    0.838116       0.432745    0.241191    0.733229       0.8468             0.350576     -0.822009    -0.408328       0.8812              
50          0.299312    -0.0399843  0.305868    0.89755        0.286049    -0.0743309  0.258888       0.9023             0.172988     -0.462719    -0.249909       0.9464              
100         0.258816    -0.0315409  0.268947    0.911917       0.241099    -0.217748   0.0604328      0.9166             0.170226     -0.763664    -0.558421       0.9424              
200         0.272165    -0.0288433  0.283057    0.903117       0.256954    -0.225211   0.0762123      0.9104             0.18663      -0.699258    -0.473622       0.9369              
300         0.239073    -0.0401494  0.232567    0.9131         0.231795    0.143831    0.409567       0.9135             0.160558     -0.170014    0.0239608       0.9435              
400         0.26256     -0.00825026  0.29052     0.9034         0.272893    -0.154971   0.159734       0.901              0.195314     -0.76846     -0.532381       0.9333              
500         0.275635    -0.0238907  0.290015    0.898967       0.263579    -0.0915541  0.211402       0.9056             0.169447     -0.531009    -0.327281       0.9422              
600         0.256046    -0.0251288  0.265498    0.9063         0.266237    -0.0867345  0.215402       0.9021             0.191472     -0.908514    -0.677984       0.9312              


$ python train.py  --gpu 0 --alg rc --baseline mean
GPU: 0
# n_units: 128
# n_hidden: 256
# Length of action sequence: 6
# RAM model: ram0
# Reinforce Algorithm: rc
# scale: 0.3
# sigma: 0.1
# baseline mode: mean
# loss coefficient: 1.0
# baseline loss coefficient: 1.0
# Optimization algorithm: adam
# Minibatch-size: 500
# epoch: 1000

epoch       main/c      main/r      main/loss   main/accuracy  val/main/c  val/main/r  val/main/loss  val/main/accuracy  test/main/c  test/main/r  test/main/loss  test/main/accuracy
1           1.91782     0.156867    2.07469     0.3104         1.65124     0.192951    1.84419        0.4028             1.49084      1.05143e-06  1.49084         0.4296              
2           1.45153     0.246169    1.6977      0.4961         1.22022     0.214318    1.43454        0.5733             1.15411      8.79669e-07  1.15411         0.5963              
5           0.944703    0.219166    1.16387     0.672783       0.898877    0.240522    1.1394         0.6898             0.813151     5.37109e-07  0.813151        0.7159              
10          0.687449    0.166651    0.8541      0.759017       0.626373    0.163101    0.789474       0.7821             0.65747      5.47791e-07  0.657471        0.7745              
20          0.541629    0.139555    0.681184    0.807433       0.498209    0.0970059   0.595215       0.8226             0.50533      3.10516e-07  0.505331        0.8258              
100         0.412645    0.109273    0.521918    0.858267       0.412371    0.120957    0.533328       0.8626             0.382748     2.39944e-07  0.382748        0.8701              
200         0.354168    0.107634    0.461801    0.875917       0.364819    0.0600111   0.42483        0.8728             0.350998     1.76239e-07  0.350998        0.88                
300         0.342124    0.103893    0.446018    0.881333       0.335684    0.0890114   0.424695       0.8843             0.318084     1.97601e-07  0.318084        0.8906              
400         0.317441    0.105628    0.423069    0.888567       0.346023    0.117242    0.463266       0.885              0.313077     2.74849e-07  0.313078        0.8925              
500         0.312834    0.113873    0.426707    0.889666       0.343479    0.0905023   0.433981       0.8799             0.325971     2.78854e-07  0.325972        0.8912              
600         0.303785    0.101987    0.405773    0.893883       0.329301    0.0938294   0.423131       0.8885             0.311558     2.07138e-07  0.311558        0.8945              



$ python train.py --gpu 0 --sigma 0.01
GPU: 0
# n_units: 128
# n_hidden: 256
# Length of action sequence: 6
# RAM model: ram0
# Reinforce Algorithm: rc
# scale: 0.3
# sigma: 0.01
# baseline mode: adaptive
# loss coefficient: 1.0
# baseline loss coefficient: 1.0
# Optimization algorithm: adam
# Minibatch-size: 500
# epoch: 1000

epoch       main/c      main/r      main/loss   main/accuracy  val/main/c  val/main/r  val/main/loss  val/main/accuracy  test/main/c  test/main/r  test/main/loss  test/main/accuracy
1           2.09645     -0.888454   1.50646     0.227567       1.7114      -0.447864   1.55901        0.4                1.70714      -0.567355    1.42873         0.3979              
2           1.3446      0.0441523   2.11487     0.530767       0.990339    1.04365     2.80955        0.6589             0.978211     1.44908      3.18016         0.6586              
5           0.629274    0.0979185   1.51887     0.7798         0.611581    -4.03158    -2.52345       0.7851             0.605349     -4.63988     -3.15541        0.7888              
10          0.56669     -0.0518228  1.29589     0.8066         0.521992    1.44765     2.6607         0.8239             0.507472     1.8151       2.99129         0.8283              
20          0.50777     -0.15674    1.14333     0.833233       0.451152    -1.07896    0.144899       0.8514             0.441157     -1.28873     -0.0686446      0.8555              
50          0.254165    0.0962119   0.746376    0.912333       0.330067    -3.65346    -2.59766       0.8912             0.332121     -4.49892     -3.42326        0.8911              
100         0.373982    -0.118893   0.841151    0.875583       0.433539    -2.45961    -1.22428       0.8542             0.434421     -2.96453     -1.69893        0.8566              
200         0.392473    -0.12539    1.00817     0.8733         0.499309    -2.38343    -0.828758      0.8434             0.493114     -2.85454     -1.31218        0.8471              
300         0.194939    0.0597188   0.578662    0.933333       0.348727    -4.52531    -3.15927       0.895              0.341365     -5.17453     -3.83042        0.8991              
400         0.106796    0.0548945   0.409307    0.968117       0.376188    -7.3622     -5.59248       0.895              0.373751     -8.52502     -6.74152        0.8966              


